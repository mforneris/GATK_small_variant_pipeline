# This is the global configuration file, all parameters should be declared in this file.
#
# We recommend to use yq to parse YAML from bash script (https://github.com/mikefarah/yq ; 
# install with brew on mac and simply use the /g/funcgen/bin/yq executable on servers)
#       then, in your script, use :
#       BWA=$(yq r global_config.yml tools.bwa)
#


# We first declare global parameters under the 'global' section 
global:
  projectpath: /Users/girardot/git/genomicsproject 
  datadir: data 
  genomedatadir: data/genome 
  seqdatadir: data/sequencing
  fastqdir: data/sequencing/fastq
  bamdir: data/sequencing/bam
  bigwigdir: data/sequencing/bw
  qcdir: data/qc
  analysisdir: analysis
  configdir: config
  envdir: env
  condaenvdir: env/conda-env
  tmpdir: tmp
  srcdir: src
  shellsrcdir: src/sh
  pysrcdir: src/python
  Rsrcdir: src/R







# TOOL PATH SECTION
# list all binaries used by your scripts in here
tools:
  bwa: /g/funcgen/bin/bwa-0.7.15
  samtools: /g/funcgen/bin/samtools-1.3.1
  embase: /g/funcgen/bin/embase
  

# expids => list the embase experiment id to sync with this project 
# expnames => for each embase exp id in expids, associate a name ; IMPORTANT integers as hash key
#             do NOT work so you must quote id with single quotes i.e. '563' not 563  
embase: 
  expids: ['563'] 
  expnames: {'563': 'my cool exp', '764': 'absjkad dsfa csd'}

# Add any additional section you might need for e.g. a snakemake pipeline ...
example: 
  anarray: ['val1', 'val2', 'val3']
  ahash: {name: 'blah girardot', surname: 'charles felix'}


# You might want to create one section per snakemake pipeline
snake_workflow1:
  snakefile: Snakefile             # snakemake file to execute
  submitToCluster: false           # should jobs go to SLURM ?
  clusterConfig: "cluster.json"    # cluster config file name, must be colocated with the snakefile
  maxRestartsPerJob: 2             # Maximum number of times a job should be reexecuted when failing 
  rerunIncomplete: true            # Rerun jobs for incomplete tasks
  maxJobsCluster: 400              # Maximum number of simultaenous jobs
  specificvar1 : value1
  specificvar2 : value2
  
  
